# ğŸ•·ï¸ SCRAPPER by BertUI
## The Session Observer for Web Automation

> **SCRAPPER isn't a scraper â€” it's a SESSION OBSERVER that captures your real browser data for use in any automation tool.**  
> Built on the BertUI React Framework  
> GitHub: [BunElysiaReact/SCRAPY](https://github.com/BunElysiaReact/SCRAPY)  
> *No domain. No cloud. All local. All yours.*

![SCRAPPER Dashboard](./finally.png)

---

## ğŸ“‹ Table of Contents
- [The Problem SCRAPPER Solves](#-the-problem-scrapper-solves)
- [What SCRAPPER Is (And Isn't)](#-what-scrapper-is-and-isnt)
- [How SCRAPPER Works](#-how-scrapper-works)
- [What SCRAPPER Captures](#-what-scrapper-captures)
- [Advantages & Disadvantages](#-advantages--disadvantages)
- [Universal Data API](#-universal-data-api)
- [Quick Start â€” Linux / macOS](#-quick-start--linux--macos)
- [Quick Start â€” Windows](#-quick-start--windows)
- [Browser Extensions](#-browser-extensions)
- [Using Captured Data in Your Tools](#-using-captured-data-in-your-tools)
- [Dashboard Overview](#-dashboard-overview)
- [Production Use & Automation](#-production-use--automation)
- [Contributing](#-contributing)

---

## ğŸ¤” The Problem SCRAPPER Solves

Every web automation tool â€” Puppeteer, Playwright, Selenium, even curl â€” shares the same challenges:

| Challenge | Why It's Hard |
|-----------|---------------|
| **Authentication** | Manually scripting logins for every site is tedious and fragile |
| **Session state** | Cookies expire, tokens rotate, localStorage gets cleared |
| **Reverse engineering** | Hours spent in DevTools understanding API patterns |
| **Bot detection** | TLS fingerprints, browser entropy, Cloudflare, hCaptcha |
| **Setup complexity** | Fighting with headless browsers, proxies, and stealth plugins |

**The real issue:** All these tools are trying to *imitate* a human. But they're guessing at what a real human looks like.

---

## ğŸ’¡ What SCRAPPER Is (And Isn't)

| SCRAPPER IS... | SCRAPPER IS NOT... |
|--------------|------------------|
| ğŸ” A **session observer** that watches YOUR real browser | âŒ A replacement for Puppeteer/Playwright/Selenium |
| ğŸ’¾ A **data capture tool** that saves your actual session | âŒ A tool that scrapes websites for you |
| ğŸ“¡ A **local API server** serving your captured data | âŒ A hosted service or cloud platform |
| ğŸ§  A **reverse engineering assistant** revealing hidden APIs | âŒ A magic "scrape anything" button |
| ğŸ¯ A **visual debugger** for understanding site structure | âŒ A no-code automation builder |

**SCRAPPER doesn't scrape. It gives you the REAL data YOU need to scrape successfully.**

---

## ğŸ”„ How SCRAPPER Works

### Phase 1: Capture (Browser Open, You Browse)

```
YOU                                              SCRAPPER
  â”‚                                                  â”‚
  â”œâ”€â”€ Open Brave/Chrome/Firefox with extension â”€â”€â”€â”€â”€â”€â–ºâ”‚
  â”‚                                                  â”‚
  â”œâ”€â”€ Log into sites you want to automate â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ captures:
  â”‚                                                  â”‚  â€¢ Cookies
  â”œâ”€â”€ Browse normally, click buttons â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  â€¢ Tokens
  â”‚                                                  â”‚  â€¢ Fingerprint
  â””â”€â”€ Done browsing â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  â€¢ API requests
                                                     â”‚  â€¢ DOM structure
```

### Phase 2: Automate (Browser Can Close, You Code)

```
YOUR SCRIPT â”€â”€â”€â”€ GET /api/v1/session/all â”€â”€â”€â”€â–º SCRAPPER API (localhost:8080)
              â—„â”€â”€ { cookies, tokens, fingerprint } â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
         Puppeteer / Playwright / Selenium / Python requests / curl
                â”‚
                â–¼
         âœ… Authenticated requests with YOUR real session
```

---

## ğŸ¬ See It In Action

### The Dashboard â€” 258 captured claude.ai requests
![SCRAPPER Dashboard Responses](./showcase.png)

### The Extension Popup â€” Live feed, quick capture, real-time stats
![SCRAPPER Extension](./extension_ui.png)

### The Installer â€” One command, 5 steps, done
![SCRAPPER Installer](./installer.png)

### Register Extension ID â€” One command after loading extension
![Register Extension](./register_extension_id_.png)

### Real World Example â€” Talking to Claude.ai from the terminal via SCRAPPER
![Claude Chat via SCRAPPER](./claudetest.png)

> The script above used SCRAPPER to capture a real claude.ai session, then sent messages directly via the API â€” zero login code, zero Puppeteer, zero browser automation.

---

## ğŸ“¦ What SCRAPPER Captures

```
ğŸ“¦ Session Data
   â”œâ”€â”€ ğŸª Cookies (including HttpOnly, Secure, all domains)
   â”œâ”€â”€ ğŸ’¾ localStorage & sessionStorage
   â”œâ”€â”€ ğŸ”‘ Auth tokens (Bearer, JWT, CSRF, custom)
   â””â”€â”€ ğŸ“¨ All HTTP headers

ğŸ–¥ï¸ Browser Fingerprint
   â”œâ”€â”€ ğŸ“± User Agent
   â”œâ”€â”€ ğŸ–¼ï¸ Screen resolution & color depth
   â”œâ”€â”€ ğŸŒ Timezone & language settings
   â””â”€â”€ ğŸ“¨ Full header set (Accept, Accept-Language, etc.)

ğŸ“¡ Network Traffic
   â”œâ”€â”€ ğŸ“¤ All HTTP requests (URLs, methods, headers, POST data)
   â”œâ”€â”€ ğŸ“¥ All HTTP responses (status, headers, bodies)
   â””â”€â”€ ğŸ”„ WebSocket frames

ğŸŒ³ DOM State
   â”œâ”€â”€ ğŸ“„ DOM snapshots
   â”œâ”€â”€ ğŸ¯ Live selector testing
   â””â”€â”€ ğŸ—ºï¸ DOM maps (all tags, classes, IDs)
```

---

## âš–ï¸ Advantages & Disadvantages

### âœ… Advantages

| Advantage | Why It Matters |
|-----------|----------------|
| **Bypasses Advanced Bot Detection** | Uses `curl_cffi` to impersonate a real browser's TLS fingerprint (e.g., Chrome 120) â€” not flagged as automated |
| **97% Success Rate** | Targets internal API routes, not visual UI â€” immune to CSS changes, moving buttons, or layout updates |
| **Low Resource Usage** | ~20MB RAM vs 500MB+ for Puppeteer/Selenium. No browser engine running |
| **Invisible Authentication** | Piggybacks off your existing human-verified session â€” no login flow, no CAPTCHAs |
| **Syncs With Real Browser** | Messages/actions from scripts appear in your real browser tab when you refresh |
| **Language Agnostic** | Session API works with Python, Go, Rust, Node, curl â€” anything that can make HTTP requests |

### âŒ Disadvantages

| Disadvantage | What It Means |
|--------------|---------------|
| **Brittle Session Lifespan** | Entirely dependent on an active browser session â€” expires if you log out |
| **Depends on Internal APIs** | Uses undocumented endpoints that can change without notice |
| **Requires Setup Infrastructure** | Not standalone â€” needs native host + browser extension running simultaneously |
| **Account Risk** | Uses your real identity. Aggressive rate-limit hitting can get your real account banned |
| **Learning Curve** | Must read network traffic to understand correct API payloads |
| **Single Device Binding** | `device-id` is tied to one captured session â€” can't easily share across machines |

---

## ğŸ“¡ Universal Data API

Once captured, SCRAPPER serves everything via a simple REST API at `http://localhost:8080`.

### Core Endpoints

| Endpoint | Description |
|----------|-------------|
| `GET /api/v1/session/cookies?domain=example.com` | All cookies for a domain |
| `GET /api/v1/session/localstorage?domain=example.com` | localStorage data |
| `GET /api/v1/session/all` | Complete session dump |
| `GET /api/v1/fingerprint` | Browser fingerprint |
| `GET /api/v1/tokens/all` | All extracted tokens |
| `GET /api/v1/requests/recent?limit=50` | Recent network requests |
| `GET /api/v1/dom/snapshot?url=example.com` | DOM snapshot |
| `GET /api/v1/export/env` | Environment variables format |
| `GET /api/v1/bulk/all?format=[json\|jsonl\|har\|csv\|txt]` | Everything, your format |

---

## ğŸš€ Quick Start â€” Linux / macOS

### One-Line Install

```bash
curl -fsSL https://raw.githubusercontent.com/BunElysiaReact/SCRAPY/main/install.sh | bash
```

### After Install â€” Complete Setup

```bash
# Step 1 â€” Load extension in Brave:
# brave://extensions â†’ Developer mode ON â†’ Load unpacked
# â†’ select: ~/.scrapper/extension/brave/

# Step 2 â€” Register your extension ID
scrapper-register-ext YOUR_EXTENSION_ID

# Step 3 â€” Start SCRAPPER
scrapper-start

# Step 4 â€” Open dashboard
# http://localhost:8080
```

### Requirements

- **Python 3** (any recent version)
- **Bun** or **Node.js** (for the Bun dashboard â€” optional)
- **Brave / Chrome / Firefox** browser

### Manual Setup

```bash
# Clone the repo
git clone https://github.com/BunElysiaReact/SCRAPY.git ~/scrapper
cd ~/scrapper

# Build the C native host
gcc -O2 -o linux/c_core/native_host/debug_host linux/c_core/native_host/debug_host.c -lpthread

# Build the Rust selector engine
cd linux/rust_finder && cargo build --release && cd ../..

# Start the API server
cd linux/python_api && python3 api.py
```

Open **http://localhost:8080** â†’ Dashboard ready.

---

## ğŸªŸ Quick Start â€” Windows

### One-Line Install (PowerShell)

```powershell
irm https://raw.githubusercontent.com/BunElysiaReact/SCRAPY/main/install.ps1 | iex
```

> âš ï¸ Run PowerShell as Administrator for native messaging registration to work correctly.

### Windows Folder Structure After Install

```
%USERPROFILE%\.scrapper\
â”œâ”€â”€ bin\
â”‚   â”œâ”€â”€ debug_host.exe         â† C native messaging host
â”‚   â”œâ”€â”€ scraper_cli.exe        â† CLI client
â”‚   â”œâ”€â”€ rust_finder.exe        â† Fast HTML selector engine
â”‚   â”œâ”€â”€ scrapper-start.bat     â† Start everything
â”‚   â””â”€â”€ scrapper-stop.bat      â† Stop everything
â”œâ”€â”€ data\                      â† All captured session data
â”œâ”€â”€ logs\                      â† Host logs
â”œâ”€â”€ python_api\
â”‚   â””â”€â”€ api.py                 â† REST API server
â”œâ”€â”€ extension\
â”‚   â”œâ”€â”€ brave\                 â† Load in Brave / Chrome / Edge
â”‚   â””â”€â”€ firefox\               â† Load in Firefox
â””â”€â”€ ui\scrapperui\             â† Dashboard source
```

---

## ğŸ§© Browser Extensions

### Brave & Chrome (Recommended)

1. Open `brave://extensions` or `chrome://extensions`
2. Enable **Developer mode** (top right)
3. Click **Load unpacked**
4. Select: `~/.scrapper/extension/brave/`
5. Copy the Extension ID
6. Run: `scrapper-register-ext YOUR_ID`

### Microsoft Edge

Same as Brave/Chrome â€” Edge is Chromium-based, use the `extension/brave/` folder.

### Firefox

1. Open `about:debugging` â†’ **This Firefox**
2. Click **Load Temporary Add-on...**
3. Select: `~/.scrapper/extension/firefox/manifest.json`

> âš ï¸ Firefox captures cookies, headers, localStorage â€” but not CDP-level request bodies.

---

## ğŸ”§ Using Captured Data in Your Tools

### Python + curl_cffi (Recommended)

```python
from curl_cffi import requests

API = "http://localhost:8080"
domain = "example.com"

cookies_raw = requests.get(f"{API}/api/v1/session/cookies?domain={domain}").json()
fp = requests.get(f"{API}/api/v1/fingerprint?domain={domain}").json()

session = requests.Session(impersonate="chrome120")
for c in cookies_raw:
    session.cookies.set(c["name"], c["value"], domain=c.get("domain", domain).lstrip("."))
session.headers.update({"User-Agent": fp.get("userAgent", "")})

response = session.get(f"https://{domain}/api/data")
print(response.json())
```

### Python requests

```python
import requests

session_data = requests.get('http://localhost:8080/api/v1/session/all').json()

s = requests.Session()
s.cookies.update({c['name']: c['value'] for c in session_data['cookies']})
s.headers.update({'User-Agent': session_data['fingerprint']['userAgent']})

response = s.get('https://api.example.com/data')
```

### curl

```bash
source <(curl -s http://localhost:8080/api/v1/export/env)

curl -X POST "https://api.example.com/upload" \
  -H "Authorization: Bearer $SCRAPPER_BEARER_TOKEN" \
  -b "$SCRAPPER_COOKIES" \
  -F "file=@document.pdf"
```

### Playwright (Python)

```python
import requests
from playwright.async_api import async_playwright

session = requests.get('http://localhost:8080/api/v1/session/all').json()

async with async_playwright() as p:
    context = await p.chromium.launch_persistent_context(
        user_data_dir="./profile",
        user_agent=session['fingerprint']['userAgent'],
    )
    await context.add_cookies(session['cookies'])
    page = await context.new_page()
    await page.goto('https://example.com')
```

### Puppeteer (Node.js)

```javascript
const session = await fetch('http://localhost:8080/api/v1/session/all').then(r => r.json());

const browser = await puppeteer.launch();
const page = await browser.newPage();
await page.setCookie(...session.cookies);
await page.setUserAgent(session.fingerprint.userAgent);
await page.goto('https://example.com/dashboard');
```

---

## ğŸ“Š Dashboard Overview

![SCRAPPER Live Dashboard](./ui_main_page.png)

Open `http://localhost:8080`:

| Tab | What It Does |
|-----|--------------|
| **Live** | Real-time stream of all captured network events |
| **Bodies** | HTTP response bodies â€” JSON, HTML, SVG, images, with preview |
| **Responses** | All HTTP responses by domain, filterable by flags |
| **Intel** | Per-domain summary â€” tokens, cookies, endpoints, DOM map |
| **Tokens** | Bearer tokens, task tokens, auth cookies, curl snippets |
| **Endpoints** | All discovered API endpoints with "Copy for LLM" |
| **DOM Map** | Full tag/class/ID tree â€” click any item to auto-scrape |
| **Find** | Test CSS selectors against real rendered HTML |
| **Nav** | Navigate + track tabs, dump cookies, capture HTML |
| **Queue** | Batch-process lists of URLs with configurable delays |

---

## âš ï¸ Realistic Expectations

### What SCRAPPER CAN Do
- âœ… Capture your REAL cookies, tokens, and fingerprint
- âœ… Save them for reuse (days to months depending on site)
- âœ… Export in JSON, JSONL, HAR, CSV, TXT formats
- âœ… Serve everything via a clean local REST API
- âœ… Help you understand how sites really work at the network level

### What SCRAPPER CANNOT Do
- âŒ Scrape websites automatically without you browsing first
- âŒ Guess what cookies or tokens look like
- âŒ Extend cookie lifetimes beyond what the site allows
- âŒ Work without the native host and extension running

---

## ğŸ­ Production Use & Automation

```bash
# Export latest session data
curl -s "http://localhost:8080/api/v1/bulk/all?format=jsonl" > session.jsonl

# Use in your scraper
python3 my-scraper.py --session session.jsonl
```

```python
# session_refresh.py â€” Weekly session refresh pipeline
import requests, schedule, time

def refresh_session():
    notify_user("Please log into target sites in your browser")
    time.sleep(300)  # 5 minutes for user to browse
    data = requests.get('http://localhost:8080/api/v1/bulk/all?format=json').json()
    with open(f'session_{int(time.time())}.json', 'w') as f:
        import json; json.dump(data, f)

schedule.every().monday.at("09:00").do(refresh_session)
while True:
    schedule.run_pending()
    time.sleep(60)
```

---

## ğŸ› ï¸ Implementation Status

### Current (v2.1.0)
- âœ… Request/response/body capture (Brave/Chrome via CDP)
- âœ… Cookie tracking (all browsers)
- âœ… DOM snapshots and selector testing
- âœ… Bearer token + task token extraction
- âœ… Browser fingerprint capture
- âœ… Live event feed (SSE)
- âœ… Bulk export (JSON/JSONL/TXT/HAR/CSV)
- âœ… URL queue with human-like delays
- âœ… "Copy for LLM" on every request and endpoint
- âœ… Brave, Chrome, Edge, Firefox extensions
- âœ… Linux + Windows installers

### Coming Soon
- ğŸ”œ Chrome Web Store listing
- ğŸ”œ Firefox Add-ons listing (signed)
- ğŸ”œ WebSocket frame capture
- ğŸ”œ Session sharing across machines

---

## ğŸ¤ Contributing

| Area | What's Needed |
|------|---------------|
| **Testing** | Try SCRAPPER on different sites, report bugs |
| **Firefox** | Help improve Firefox extension CDP workarounds |
| **Windows** | Test Windows installer edge cases |
| **Docs** | Write tutorials for specific sites or use cases |
| **Code** | PRs welcome â€” especially bug fixes |

[Open an issue](https://github.com/BunElysiaReact/SCRAPY/issues)

---

## ğŸ™ Built With

- **BertUI React Framework** â€” Dashboard UI
- **Bun + ElysiaJS** â€” Fast JavaScript runtime
- **Rust + scraper crate** â€” Blazing-fast CSS selector engine
- **C** â€” Ultra-low-latency native messaging host (Linux)
- **C + WinAPI** â€” Native messaging host (Windows named pipes)
- **Python 3** â€” Zero-dependency REST API server

---

*SCRAPPER by BertUI â€” The Session Observer for Web Automation*  
*ğŸ” Watching your browser so you don't have to*

**â­ Star the repo if SCRAPPER helps you â€” it helps others find it!**
```
