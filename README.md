# ğŸ•·ï¸ SCRAPY by BertUI
> **The browser-native web scraper that actually works.**  
> Built on the BertUI React Framework Â· v2.1.0  
> GitHub: [BunElysiaReact/SCRAPY](https://github.com/BunElysiaReact/SCRAPY)

---

Scrapy isn't just another scraper. It runs **inside your browser** â€” meaning it inherits your login session, your cookies, your identity â€” and looks exactly like a real human to every anti-bot system ever built. No IP bans. No CAPTCHAs. No rate limits from hell. Just pure, unfiltered data.

While every other scraper is out there faking HTTP headers and getting blocked instantly, Scrapy is sitting inside your browser silently capturing everything â€” API responses, auth tokens, WebSocket frames, cookies, DOM maps â€” in real time, with zero configuration.

---

## âœ¨ What Makes Scrapy Different

- **Runs inside your actual browser session** â€” it IS you, to every website
- **Captures everything** â€” requests, responses, JSON bodies, cookies, localStorage tokens, WebSocket frames
- **Stealth by default** â€” fingerprint jitter, human scroll/mouse simulation, canvas noise, WebGL spoofing
- **CSS selector extraction** â€” powered by a blazing-fast Rust engine
- **Beautiful React dashboard** â€” live event stream, token extraction, DOM map, batch queue
- **C native host backend** â€” ultra-low latency, zero overhead, saves everything to disk as JSONL

---

## ğŸ–¥ï¸ Platform Support

| Platform | Status |
|----------|--------|
| ğŸ§ Linux (MX Linux, Ubuntu 18+, Debian) | âœ… Fully tested |
| ğŸªŸ Windows 10/11 | âš ï¸ Compiled â€” not yet tested (contributions welcome!) |
| ğŸ macOS | âŒ Not supported (no Mac to test on â€” PRs welcome!) |

> Windows support is theoretically complete â€” the `.exe` binaries are cross-compiled from Linux. It has not been tested on a real Windows machine. If you're on Windows, try it and open an issue!

---

## ğŸŒ Browser Compatibility

| Browser | Status |
|---------|--------|
| Brave | âœ… Tested |
| Google Chrome | âœ… Should work |
| Microsoft Edge | âœ… Should work |
| Opera / Vivaldi | âœ… Should work |
| Firefox | ğŸ”œ Coming soon â€” Firefox support is in active development |
| Safari | âŒ Not supported |

> **macOS note:** No Mac available for testing so macOS support isn't something that can be guaranteed or maintained. If you're on macOS and want to try it, you're welcome to â€” but you're on your own. PRs with macOS fixes are welcome.

---

## âš”ï¸ Scrapy vs Puppeteer (+ plugins)

Everyone reaches for Puppeteer first. Here's why that's the wrong call for serious scraping:

| Feature | Puppeteer + Plugins | Scrapy |
|---------|-------------------|--------|
| **Runs in real browser session** | âŒ Spawns a separate browser, no existing session | âœ… Runs inside YOUR browser â€” you're already logged in |
| **Auth / login state** | âŒ You have to script the login every time | âœ… Inherited automatically â€” it IS your session |
| **Bot detection bypass** | âš ï¸ Needs puppeteer-extra-plugin-stealth + constant maintenance as sites update | âœ… Undetectable by design â€” same fingerprint as you |
| **Anti-bot / Cloudflare** | âŒ Frequently blocked, needs proxies + residential IPs | âœ… You're a real user â€” no blocks |
| **Setup complexity** | âŒ Node.js + puppeteer + stealth plugin + proxy config + sometimes a full headless server | âœ… Load extension + run two commands |
| **Captures WebSocket frames** | âš ï¸ Possible but complex | âœ… Built-in, automatic |
| **Captures auth tokens** | âš ï¸ Requires intercepting requests manually | âœ… Automatic â€” saved to auth.jsonl |
| **CSS selector extraction** | âœ… Yes | âœ… Yes â€” via Rust engine (faster) |
| **Live data stream** | âŒ No dashboard | âœ… Real-time dashboard at localhost:3000 |
| **Resource usage** | âŒ Heavy â€” launches a full browser process | âœ… Lightweight â€” piggybacks your existing browser |
| **Maintained session cookies** | âŒ Expires, needs re-authentication scripts | âœ… Stays logged in as long as you are |
| **JavaScript-heavy SPAs** | âš ï¸ Works but requires waiting for network idle | âœ… You browse it like a human â€” it just records |
| **Language** | JavaScript / Node.js | Any â€” API is HTTP, CLI is a terminal |

### The real difference

Puppeteer pretends to be a human. Scrapy **is** a human â€” you. No amount of stealth plugins will make Puppeteer as undetectable as an actual browser with an actual session that's been logged into a site for months. Sites check cookie age, session history, behavioral patterns, TLS fingerprints, and dozens of other signals. Puppeteer fakes all of them. Scrapy doesn't need to fake any of them.

For scraping sites that don't require auth and don't have serious bot detection, Puppeteer is fine. For anything serious â€” paywalled content, sites behind Cloudflare, anything that requires being logged in â€” Scrapy is in a completely different league.

---

## ğŸ“¦ What's in the Release

```
scrapy-[platform]/
â”œâ”€â”€ c_core/native_host/     â† Pre-compiled C binaries (debug_host, scraper_cli)
â”œâ”€â”€ rust_finder/            â† Pre-compiled Rust element extractor (finder)
â”œâ”€â”€ extension/brave/        â† Browser extension (load unpacked)
â”œâ”€â”€ python_api/             â† API server â€” api.py (no pip deps needed!)
â”œâ”€â”€ ui/scrapperui/          â† React dashboard (BertUI framework)
â”œâ”€â”€ config/                 â† Native messaging manifest
â”œâ”€â”€ src/                    â† Source code (for developers)
â”œâ”€â”€ data/                   â† Captured data goes here (.jsonl files)
â””â”€â”€ logs/                   â† Debug logs
```

> **Users do NOT need Rust or GCC.** Binaries are pre-compiled and included. You only need **Python 3** and **Bun**.

---

## 1. Installation

> â±ï¸ **The setup is a one-time process.** Yes, it takes 10â€“15 minutes. But you only ever do it once â€” after that, Scrapy just works every time you open your browser.

---

### i) ğŸ§ Linux Installation

#### Step 1 â€” Extract the release

```bash
tar -xzf scrapy-linux-x64.tar.gz
cd scrapy-linux-x64
```

#### Step 2 â€” Make binaries executable

```bash
chmod +x c_core/native_host/debug_host
chmod +x c_core/native_host/scraper_cli
chmod +x rust_finder/target/release/finder
```

#### Step 3 â€” Register the Native Messaging Host

Edit the config:
```bash
nano config/com.scraper.core.json
```

Set the `path` to the absolute path of `debug_host` on your machine:
```json
{
  "name": "com.scraper.core",
  "description": "Scraper Core Native Host",
  "path": "/home/YOUR_USERNAME/scrapy-linux-x64/c_core/native_host/debug_host",
  "type": "stdio",
  "allowed_origins": ["chrome-extension://YOUR_EXTENSION_ID_HERE/"]
}
```

> âš ï¸ Replace `YOUR_USERNAME` with your Linux username. Extension ID comes in Step 5.

Copy to your browser's native messaging folder:

**Brave:**
```bash
mkdir -p ~/.config/BraveSoftware/Brave-Browser/NativeMessagingHosts/
cp config/com.scraper.core.json ~/.config/BraveSoftware/Brave-Browser/NativeMessagingHosts/
```

**Chrome:**
```bash
mkdir -p ~/.config/google-chrome/NativeMessagingHosts/
cp config/com.scraper.core.json ~/.config/google-chrome/NativeMessagingHosts/
```

**Edge:**
```bash
mkdir -p ~/.config/microsoft-edge/NativeMessagingHosts/
cp config/com.scraper.core.json ~/.config/microsoft-edge/NativeMessagingHosts/
```

#### Step 4 â€” Load the Extension

1. Open `brave://extensions`
2. Toggle ON **Developer mode** (top-right)
3. Click **Load unpacked** â†’ select `extension/brave/`
4. Copy the **Extension ID** (long string under the extension name)

#### Step 5 â€” Update Config with Extension ID

```bash
nano config/com.scraper.core.json
```

Replace `YOUR_EXTENSION_ID_HERE` with your copied ID:
```json
"allowed_origins": ["chrome-extension://iodcmibmbgancdcommocomjgalgdmpml/"]
```

> âš ï¸ The trailing slash is required.

Re-copy the manifest, then reload the extension:
```bash
cp config/com.scraper.core.json ~/.config/BraveSoftware/Brave-Browser/NativeMessagingHosts/
```

Go to `brave://extensions` â†’ click **Reload** on the Scrapy card.

---

### ii) ğŸªŸ Windows Installation

> âš ï¸ **Windows support is theoretical â€” the `.exe` binaries are cross-compiled from Linux but have not been tested on a real Windows machine.** Try it and open an issue on GitHub with results!

#### Step 1 â€” Extract

Extract `scrapy-windows-x64.zip` to a permanent location, e.g.:
```
C:\scrapy\scrapy-windows-x64\
```

#### Step 2 â€” Register the Native Messaging Host

Edit `config\com.scraper.core.json`:
```json
{
  "name": "com.scraper.core",
  "description": "Scraper Core Native Host",
  "path": "C:\\scrapy\\scrapy-windows-x64\\c_core\\native_host\\debug_host.exe",
  "type": "stdio",
  "allowed_origins": ["chrome-extension://YOUR_EXTENSION_ID_HERE/"]
}
```

Copy it (run PowerShell as Administrator):

**Brave:**
```powershell
mkdir "$env:LOCALAPPDATA\BraveSoftware\Brave-Browser\NativeMessagingHosts\" -Force
copy config\com.scraper.core.json "$env:LOCALAPPDATA\BraveSoftware\Brave-Browser\NativeMessagingHosts\"
```

**Chrome:**
```powershell
mkdir "$env:LOCALAPPDATA\Google\Chrome\User Data\NativeMessagingHosts\" -Force
copy config\com.scraper.core.json "$env:LOCALAPPDATA\Google\Chrome\User Data\NativeMessagingHosts\"
```

#### Step 3 â€” Load the Extension

1. Open `brave://extensions`
2. Enable **Developer mode**
3. Click **Load unpacked** â†’ select `extension\brave\`
4. Copy the **Extension ID**
5. Paste it into `com.scraper.core.json`, re-copy the manifest, reload the extension

---

### iii) ğŸ§ª Tester / Quick Start

If you just want to run and test Scrapy â€” no compilers needed.

**Requirements: Python 3 and Bun. That's it.**

Install Bun (one-time):
```bash
# Linux/macOS
curl -fsSL https://bun.sh/install | bash

# Windows: download from https://bun.sh
```

Run Scrapy with 3 terminals:

**Terminal 1 â€” API server:**
```bash
cd python_api
python3 api.py          # Linux
python api.py           # Windows
```

**Terminal 2 â€” Dashboard:**
```bash
cd ui/scrapperui
bun install             # first time only
bun run dev
```

**Terminal 3 â€” CLI (optional live feed):**
```bash
./c_core/native_host/scraper_cli        # Linux
.\c_core\native_host\scraper_cli.exe    # Windows
```

Open **http://localhost:3000** â€” you're in.

Quick test to verify everything works:
1. Go to the **Find** tab
2. URL: `https://books.toscrape.com`
3. Selector: `p.price_color`
4. Hit **Scrape** â†’ you'll get prices back as JSON instantly

---

### iv) ğŸ› ï¸ Developer Setup

Build from source, modify, or contribute.

**Requirements:** GCC Â· Rust + Cargo Â· Python 3 Â· Bun

```bash
# Install all build tools (Linux/Debian)
sudo apt install gcc build-essential -y
curl https://sh.rustup.rs -sSf | sh && source ~/.cargo/env
curl -fsSL https://bun.sh/install | bash

# Clone the repo
git clone git@github.com:BunElysiaReact/SCRAPY.git
cd SCRAPY
```

**Build the C native host:**
```bash
cd c_core/native_host
gcc -o debug_host  debug_host.c  -lpthread
gcc -o scraper_cli scraper_cli.c
```

**Build the Rust finder:**
```bash
cd rust_finder
cargo build --release
# Binary â†’ target/release/finder (or name in Cargo.toml)
```

**Run the UI in dev mode:**
```bash
cd ui/scrapperui
bun install
bun run dev
```

**Build release packages (Linux + Windows):**
```bash
chmod +x create_releases.sh
./create_releases.sh
```

> Cross-compile for Windows requires: `sudo apt install mingw-w64 -y`

---

## â–¶ï¸ Running Scrapy

Three terminals, always running together:

| Terminal | Command (Linux) |
|----------|-----------------|
| 1 â€” API | `cd python_api && python3 api.py` |
| 2 â€” Dashboard | `cd ui/scrapperui && bun run dev` |
| 3 â€” CLI (optional) | `./c_core/native_host/scraper_cli` |

Open **http://localhost:3000**

---

## ğŸ¯ Using Scrapy

**Track a Tab â€” capture everything live:**
1. Open any website in Brave
2. Navigate tab â†’ click **Track Tab**
3. Browse normally â€” all requests, responses, tokens, cookies captured

**Scrape Elements â€” instant, no tracking needed:**
1. Find tab â†’ enter URL + CSS selector â†’ **Scrape**
2. Results come back as JSON

**Example selectors:**
```
h3 a                 â†’ link titles
p.price_color        â†’ prices on books.toscrape.com
span.titleline a     â†’ Hacker News post titles
article.product_pod  â†’ full product cards
```

---

## ğŸ“Š Dashboard Tabs

| Tab | Description |
|-----|-------------|
| Live | Real-time event stream |
| Responses | All HTTP responses by domain |
| Intel | Tokens, cookies, API endpoints summary |
| Tokens | Extracted bearer tokens + auth cookies |
| Endpoints | All discovered API endpoints |
| DOM Map | Full tag/class/ID map of the page |
| Find | CSS selector scraper â†’ JSON |
| Navigate | Track tabs, dump cookies, get HTML |
| Queue | Batch-process a list of URLs |

---

## ğŸ’¾ Data Files

Saved to `data/` as `.jsonl` (one JSON object per line):

```
requests.jsonl      â†’ Flagged HTTP requests
responses.jsonl     â†’ All HTTP responses
bodies.jsonl        â†’ Response bodies
auth.jsonl          â†’ Auth cookies + localStorage tokens
cookies.jsonl       â†’ All cookies
websockets.jsonl    â†’ WebSocket frames
dommaps.jsonl       â†’ DOM snapshots
```

---

## ğŸ”§ Troubleshooting

**Extension not connecting:**
- Check Extension ID in manifest matches exactly
- Verify `path` points to real binary
- Manifest must be in the correct NativeMessagingHosts folder
- Reload extension after every manifest change
- Check: `tail -f logs/debug_host.log`

**Dashboard OFFLINE:**
- Make sure `api.py` is running
- `lsof -i :8080` to check port

**Port conflict:**
```bash
pkill -f api.py && python3 api.py
```

**No events in Live tab:**
- Click **Track Tab** before browsing
- Old HTML sites have no API calls â€” use **Find** tab instead

---

## ğŸ“¬ Contributing

PRs welcome! Tested it on Windows or macOS? Open an issue with your results.

[https://github.com/BunElysiaReact/SCRAPY](https://github.com/BunElysiaReact/SCRAPY)

---

*Scrapy â€” Made by BertUI Â· BertUI React Framework Â· v2.1.0*  
*Tested on Brave v145 (Chromium) on MX Linux / Ubuntu*