# ğŸ•·ï¸ SCRAPPER by BertUI
## The Session Observer for Web Automation

> **SCRAPPER isn't a scraper â€” it's a SESSION OBSERVER that captures your real browser data for use in any automation tool.**  
> Built on the BertUI React Framework  
> GitHub: [BunElysiaReact/SCRAPY](https://github.com/BunElysiaReact/SCRAPY)  
> *No domain. No cloud. All local. All yours.*

---

## ğŸ“‹ Table of Contents
- [The Problem SCRAPPER Solves](#-the-problem-scrapper-solves)
- [What SCRAPPER Is (And Isn't)](#-what-scrapper-is-and-isnt)
- [How SCRAPPER Works](#-how-scrapper-works)
- [What SCRAPPER Captures](#-what-scrapper-captures)
- [Advantages & Disadvantages](#-advantages--disadvantages)
- [Universal Data API](#-universal-data-api)
- [Quick Start â€” Linux / macOS](#-quick-start--linux--macos)
- [Quick Start â€” Windows](#-quick-start--windows)
- [Browser Extensions](#-browser-extensions)
- [Using Captured Data in Your Tools](#-using-captured-data-in-your-tools)
- [Dashboard Overview](#-dashboard-overview)
- [Production Use & Automation](#-production-use--automation)
- [Contributing](#-contributing)

---

## ğŸ¤” The Problem SCRAPPER Solves

Every web automation tool â€” Puppeteer, Playwright, Selenium, even curl â€” shares the same challenges:

| Challenge | Why It's Hard |
|-----------|---------------|
| **Authentication** | Manually scripting logins for every site is tedious and fragile |
| **Session state** | Cookies expire, tokens rotate, localStorage gets cleared |
| **Reverse engineering** | Hours spent in DevTools understanding API patterns |
| **Bot detection** | TLS fingerprints, browser entropy, Cloudflare, hCaptcha |
| **Setup complexity** | Fighting with headless browsers, proxies, and stealth plugins |

**The real issue:** All these tools are trying to *imitate* a human. But they're guessing at what a real human looks like.

---

## ğŸ’¡ What SCRAPPER Is (And Isn't)

| SCRAPPER IS... | SCRAPPER IS NOT... |
|--------------|------------------|
| ğŸ” A **session observer** that watches YOUR real browser | âŒ A replacement for Puppeteer/Playwright/Selenium |
| ğŸ’¾ A **data capture tool** that saves your actual session | âŒ A tool that scrapes websites for you |
| ğŸ“¡ A **local API server** serving your captured data | âŒ A hosted service or cloud platform |
| ğŸ§  A **reverse engineering assistant** revealing hidden APIs | âŒ A magic "scrape anything" button |
| ğŸ¯ A **visual debugger** for understanding site structure | âŒ A no-code automation builder |

**SCRAPPER doesn't scrape. It gives you the REAL data YOU need to scrape successfully.**

---

## ğŸ”„ How SCRAPPER Works

### Phase 1: Capture (Browser Open, You Browse)

```
YOU                                              SCRAPPER
  â”‚                                                  â”‚
  â”œâ”€â”€ Open Brave/Chrome/Firefox with extension â”€â”€â”€â”€â”€â”€â–ºâ”‚
  â”‚                                                  â”‚
  â”œâ”€â”€ Log into sites you want to automate â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ captures:
  â”‚                                                  â”‚  â€¢ Cookies
  â”œâ”€â”€ Browse normally, click buttons â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  â€¢ Tokens
  â”‚                                                  â”‚  â€¢ Fingerprint
  â””â”€â”€ Done browsing â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  â€¢ API requests
                                                     â”‚  â€¢ DOM structure
```

### Phase 2: Automate (Browser Can Close, You Code)

```
YOUR SCRIPT â”€â”€â”€â”€ GET /api/v1/session/all â”€â”€â”€â”€â–º SCRAPPER API (localhost:8080)
              â—„â”€â”€ { cookies, tokens, fingerprint } â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
         Puppeteer / Playwright / Selenium / Python requests / curl
                â”‚
                â–¼
         âœ… Authenticated requests with YOUR real session
```

---

## ğŸ“¦ What SCRAPPER Captures

```
ğŸ“¦ Session Data
   â”œâ”€â”€ ğŸª Cookies (including HttpOnly, Secure, all domains)
   â”œâ”€â”€ ğŸ’¾ localStorage & sessionStorage
   â”œâ”€â”€ ğŸ”‘ Auth tokens (Bearer, JWT, CSRF, custom)
   â””â”€â”€ ğŸ“¨ All HTTP headers

ğŸ–¥ï¸ Browser Fingerprint
   â”œâ”€â”€ ğŸ“± User Agent
   â”œâ”€â”€ ğŸ–¼ï¸ Screen resolution & color depth
   â”œâ”€â”€ ğŸŒ Timezone & language settings
   â””â”€â”€ ğŸ“¨ Full header set (Accept, Accept-Language, etc.)

ğŸ“¡ Network Traffic
   â”œâ”€â”€ ğŸ“¤ All HTTP requests (URLs, methods, headers, POST data)
   â”œâ”€â”€ ğŸ“¥ All HTTP responses (status, headers, bodies)
   â””â”€â”€ ğŸ”„ WebSocket frames

ğŸŒ³ DOM State
   â”œâ”€â”€ ğŸ“„ DOM snapshots
   â”œâ”€â”€ ğŸ¯ Live selector testing
   â””â”€â”€ ğŸ—ºï¸ DOM maps (all tags, classes, IDs)
```

---

## âš–ï¸ Advantages & Disadvantages

### âœ… Advantages

| Advantage | Why It Matters |
|-----------|----------------|
| **Bypasses Advanced Bot Detection** | Uses `curl_cffi` to impersonate a real browser's TLS fingerprint (e.g., Chrome 120) â€” not flagged as automated |
| **97% Success Rate** | Targets internal API routes, not visual UI â€” immune to CSS changes, moving buttons, or layout updates |
| **Low Resource Usage** | ~20MB RAM vs 500MB+ for Puppeteer/Selenium. No browser engine running |
| **Invisible Authentication** | Piggybacks off your existing human-verified session â€” no login flow, no CAPTCHAs |
| **Syncs With Real Browser** | Messages/actions from scripts appear in your real browser tab when you refresh |
| **Language Agnostic** | Session API works with Python, Go, Rust, Node, curl â€” anything that can make HTTP requests |

### âŒ Disadvantages

| Disadvantage | What It Means |
|--------------|---------------|
| **Brittle Session Lifespan** | Entirely dependent on an active browser session â€” expires if you log out |
| **Depends on Internal APIs** | Uses undocumented endpoints that can change without notice |
| **Requires Setup Infrastructure** | Not standalone â€” needs debug host + browser extension running simultaneously |
| **Account Risk** | Uses your real identity. Aggressive rate-limit hitting can get your real account banned |
| **Learning Curve** | Must read network traffic to understand correct API payloads |
| **Single Device Binding** | `device-id` is tied to one captured session â€” can't easily share across machines |

---

## ğŸ“¡ Universal Data API

Once captured, SCRAPPER serves everything via a simple REST API at `http://localhost:8080`.

### Core Endpoints

| Endpoint | Description |
|----------|-------------|
| `GET /api/v1/session/cookies?domain=example.com` | All cookies for a domain |
| `GET /api/v1/session/localstorage?domain=example.com` | localStorage data |
| `GET /api/v1/session/all` | Complete session dump |
| `GET /api/v1/fingerprint` | Browser fingerprint |
| `GET /api/v1/tokens/all` | All extracted tokens |
| `GET /api/v1/requests/recent?limit=50` | Recent network requests |
| `GET /api/v1/dom/snapshot?url=example.com` | DOM snapshot |
| `GET /api/v1/export/env` | Environment variables format |
| `GET /api/v1/bulk/all?format=[json\|jsonl\|har\|csv\|txt]` | Everything, your format |

---

## ğŸš€ Quick Start â€” Linux / macOS

### One-Line Install

```bash
curl -fsSL https://raw.githubusercontent.com/BunElysiaReact/SCRAPY/main/install.sh | bash
```

### Requirements

- **Python 3** (any recent version)
- **Bun** or **Node.js** (for the dashboard)
- **Brave / Chrome / Firefox** browser

### Manual Setup

```bash
# Clone the repo
git clone https://github.com/BunElysiaReact/SCRAPY.git ~/scrapper
cd ~/scrapper

# Build the C native host
gcc -O2 -o c_core/native_host/debug_host c_core/native_host/debug_host.c -lpthread
gcc -O2 -o c_core/native_host/scraper_cli c_core/native_host/scraper_cli.c

# Build the Rust selector engine
cd rust_finder && cargo build --release && cd ..

# Start the API server
cd python_api && python3 api.py

# In another terminal â€” start the dashboard
cd ui/scrapperui && bun install && bun run dev
```

Open **http://localhost:3000** â†’ Dashboard ready.

---

## ğŸªŸ Quick Start â€” Windows

### One-Line Install (PowerShell)

```powershell
irm https://raw.githubusercontent.com/BunElysiaReact/SCRAPY/main/install.ps1 | iex
```

### What the installer does on Windows

1. Downloads pre-compiled `debug_host.exe` and `scraper_cli.exe`
2. Downloads pre-compiled `rust_finder.exe`
3. Installs the browser extension for Brave/Chrome/Edge
4. Registers the native messaging manifest in the correct location:
   - Brave: `%APPDATA%\BraveSoftware\Brave-Browser\NativeMessagingHosts\`
   - Chrome: `%APPDATA%\Google\Chrome\NativeMessagingHosts\`
   - Edge: `%APPDATA%\Microsoft\Edge\NativeMessagingHosts\`
5. Creates `scrapper-start.bat` and `scrapper-stop.bat` in `%USERPROFILE%\.scrapper\bin\`

### Manual Windows Setup

```powershell
# Requires: Python 3, Bun or Node, mingw64 (for compiling C), Rust (for compiling Rust)

git clone https://github.com/BunElysiaReact/SCRAPY.git $env:USERPROFILE\.scrapper
cd $env:USERPROFILE\.scrapper

# Compile C host (requires mingw64 or MSVC)
x86_64-w64-mingw32-gcc -o c_core\native_host\debug_host.exe c_core\native_host\debug_host_win.c -D_WIN32_WINNT=0x0600

# Compile Rust
cd rust_finder; cargo build --release; cd ..

# Register native messaging (update path and extension ID first)
# Edit config\com.scraper.core.json then copy to:
# %APPDATA%\BraveSoftware\Brave-Browser\NativeMessagingHosts\com.scraper.core.json

# Start the API
python python_api\api.py

# Start the dashboard
cd ui\scrapperui && bun install && bun run dev
```

### Windows Folder Structure After Install

```
%USERPROFILE%\.scrapper\
â”œâ”€â”€ bin\
â”‚   â”œâ”€â”€ debug_host.exe         â† C native messaging host
â”‚   â”œâ”€â”€ scraper_cli.exe        â† CLI client
â”‚   â”œâ”€â”€ rust_finder.exe        â† Fast HTML selector engine
â”‚   â”œâ”€â”€ scrapper-start.bat     â† Start everything
â”‚   â””â”€â”€ scrapper-stop.bat      â† Stop everything
â”œâ”€â”€ data\                      â† All captured session data
â”œâ”€â”€ logs\                      â† Host logs
â”œâ”€â”€ python_api\
â”‚   â””â”€â”€ api.py                 â† REST API server
â”œâ”€â”€ extension\
â”‚   â”œâ”€â”€ brave\                 â† Load in Brave / Chrome / Edge
â”‚   â””â”€â”€ firefox\               â† Load in Firefox
â””â”€â”€ ui\scrapperui\             â† Dashboard source
```

---

## ğŸ§© Browser Extensions

SCRAPPER has extensions for all major browsers. Load the extension folder **unpacked** in developer mode.

### Brave & Chrome (Recommended)

> Brave and Chrome share the same Chromium engine. **Use the same extension folder for both.**

1. Open `brave://extensions` or `chrome://extensions`
2. Enable **Developer mode** (top right)
3. Click **Load unpacked**
4. Select: `extension/brave/`

Full features: CDP debugging, DOM mapping, fingerprint capture, stealth injection, live feed, popup UI.

### Microsoft Edge

1. Open `edge://extensions`
2. Enable **Developer mode**
3. Click **Load unpacked**
4. Select: `extension/brave/` (same folder â€” Edge is Chromium-based)

### Firefox

> Firefox uses Manifest V2 and does not support the Chrome Debugger Protocol.  
> The Firefox extension captures cookies, network headers, and localStorage â€” but **not** CDP-level request bodies.

1. Open `about:debugging`
2. Click **This Firefox**
3. Click **Load Temporary Add-on...**
4. Select: `extension/firefox/manifest.json`

For permanent installation, sign the extension at [addons.mozilla.org](https://addons.mozilla.org/developers/).

**Firefox captures:**
- âœ… All cookies (including auth cookies)
- âœ… Cookie change events
- âœ… localStorage / sessionStorage (via content script)
- âœ… Basic network request/response logging
- âš ï¸ No response bodies (Firefox API limitation without CDP)
- âš ï¸ No DOM mapping or fingerprint capture

### After Loading â€” Register Your Extension ID

After loading the extension, copy its ID from the extensions page, then run:

**Linux/macOS:**
```bash
scrapy-register-ext YOUR_EXTENSION_ID_HERE
```

**Windows:**
```powershell
scrapper-register-ext.ps1 YOUR_EXTENSION_ID_HERE
```

---

## ğŸ”§ Using Captured Data in Your Tools

### Python Requests

```python
import requests

session_data = requests.get('http://localhost:8080/api/v1/session/all').json()

s = requests.Session()
s.cookies.update({c['name']: c['value'] for c in session_data['cookies']})
s.headers.update({'User-Agent': session_data['fingerprint']['userAgent']})

response = s.get('https://api.example.com/data')
```

### curl

```bash
source <(curl -s http://localhost:8080/api/v1/export/env)

curl -X POST "https://api.example.com/upload" \
  -H "Authorization: Bearer $SCRAPY_BEARER_TOKEN" \
  -b "$SCRAPY_COOKIES" \
  -F "file=@document.pdf"
```

### Playwright (Python)

```python
import requests
from playwright.async_api import async_playwright

session = requests.get('http://localhost:8080/api/v1/session/all').json()

async with async_playwright() as p:
    context = await p.chromium.launch_persistent_context(
        user_data_dir="./profile",
        user_agent=session['fingerprint']['userAgent'],
    )
    await context.add_cookies(session['cookies'])
    page = await context.new_page()
    await page.goto('https://example.com')
```

### Puppeteer (Node.js)

```javascript
const session = await fetch('http://localhost:8080/api/v1/session/all').then(r => r.json());

const browser = await puppeteer.launch();
const page = await browser.newPage();
await page.setCookie(...session.cookies);
await page.setUserAgent(session.fingerprint.userAgent);
await page.goto('https://example.com/dashboard');
```

---

## ğŸ“Š Dashboard Overview

Open `http://localhost:3000` (dev) or `http://localhost:8080` (production):

| Tab | What It Does |
|-----|--------------|
| **Live** | Real-time stream of all captured network events |
| **Bodies** | HTTP response bodies â€” JSON, HTML, SVG, images, with preview |
| **Responses** | All HTTP responses by domain, filterable by flags |
| **Intel** | Per-domain summary â€” tokens, cookies, endpoints, DOM map |
| **Tokens** | Bearer tokens, task tokens, auth cookies, curl snippets |
| **Endpoints** | All discovered API endpoints with "Copy for LLM" |
| **DOM Map** | Full tag/class/ID tree â€” click any item to auto-scrape |
| **Find** | Test CSS selectors against real rendered HTML |
| **Nav** | Navigate + track tabs, dump cookies, capture HTML |
| **Queue** | Batch-process lists of URLs with configurable delays |

---

## âš ï¸ Realistic Expectations

### What SCRAPPER CAN Do
- âœ… Capture your REAL cookies, tokens, and fingerprint
- âœ… Save them for reuse (days to months depending on site)
- âœ… Export in JSON, JSONL, HAR, CSV, TXT formats
- âœ… Serve everything via a clean local REST API
- âœ… Help you understand how sites really work at the network level

### What SCRAPPER CANNOT Do
- âŒ Scrape websites automatically without you browsing first
- âŒ Guess what cookies or tokens look like
- âŒ Extend cookie lifetimes beyond what the site allows
- âŒ Work without the native host and extension running

---

## ğŸ­ Production Use & Automation

```bash
# Export latest session data
curl -s "http://localhost:8080/api/v1/bulk/all?format=jsonl" > session.jsonl

# Use in your scraper
python3 my-scraper.py --session session.jsonl
```

```python
# session_refresh.py â€” Weekly session refresh pipeline
import requests, schedule, time

def refresh_session():
    notify_user("Please log into target sites in your browser")
    time.sleep(300)  # 5 minutes for user to browse
    data = requests.get('http://localhost:8080/api/v1/bulk/all?format=json').json()
    with open(f'session_{int(time.time())}.json', 'w') as f:
        import json; json.dump(data, f)

schedule.every().monday.at("09:00").do(refresh_session)
while True:
    schedule.run_pending()
    time.sleep(60)
```

---

## ğŸ› ï¸ Implementation Status

### Current (v2.1.0)
- âœ… Request/response/body capture (Brave/Chrome via CDP)
- âœ… Cookie tracking (all browsers)
- âœ… DOM snapshots and selector testing
- âœ… Bearer token + task token extraction
- âœ… Browser fingerprint capture
- âœ… Live event feed (SSE)
- âœ… Bulk export (JSON/JSONL/TXT/HAR/CSV)
- âœ… URL queue with human-like delays
- âœ… "Copy for LLM" on every request and endpoint
- âœ… Brave, Chrome, Edge, Firefox extensions
- âœ… Linux + Windows installers

### Coming Soon
- ğŸ”œ Chrome Web Store listing
- ğŸ”œ Firefox Add-ons listing (signed)
- ğŸ”œ WebSocket frame capture
- ğŸ”œ Session sharing across machines

---

## ğŸ¤ Contributing

### Ways to Contribute

| Area | What's Needed |
|------|---------------|
| **Testing** | Try SCRAPPER on different sites, report bugs |
| **Firefox** | Help improve Firefox extension CDP workarounds |
| **Windows** | Test Windows installer edge cases |
| **Docs** | Write tutorials for specific sites or use cases |
| **Code** | PRs welcome â€” especially bug fixes |

### Report Issues
[Open an issue](https://github.com/BunElysiaReact/SCRAPY/issues)

---

## ğŸ“¬ Get Involved

- **GitHub**: [BunElysiaReact/SCRAPY](https://github.com/BunElysiaReact/SCRAPY)
- **Issues**: [github.com/BunElysiaReact/SCRAPY/issues](https://github.com/BunElysiaReact/SCRAPY/issues)

---

## ğŸ™ Built With

- **BertUI React Framework** â€” Dashboard UI
- **Bun + ElysiaJS** â€” Fast JavaScript runtime
- **Rust + scraper crate** â€” Blazing-fast CSS selector engine
- **C** â€” Ultra-low-latency native messaging host (Linux)
- **C + WinAPI** â€” Native messaging host (Windows named pipes)
- **Python 3** â€” Zero-dependency REST API server

---

*SCRAPPER by BertUI â€” The Session Observer for Web Automation*  
*ğŸ” Watching your browser so you don't have to*

**â­ Star the repo if SCRAPPER helps you â€” it helps others find it!**